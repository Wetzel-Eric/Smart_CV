{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2df41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "410593c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import tempfile\n",
    "from typing import Any\n",
    "import pickle\n",
    "\n",
    "from langchain_community.document_loaders.epub import UnstructuredEPubLoader\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders.word_document import (UnstructuredWordDocumentLoader)\n",
    "from langchain_core.documents import Document\n",
    "from streamlit.logger import get_logger\n",
    "\n",
    "logging.basicConfig(encoding=\"utf-8\", level=logging.INFO)\n",
    "LOGGER = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8d760",
   "metadata": {},
   "source": [
    "#Custom class loader: La classe Epubreader herité de UnstructuredEPubLoader et le configure en \"fast\" mode pour l'extraction.\n",
    "\n",
    "#DocumentLoader class: Classe centrale qui manage le chargement de fichier selon differents formats\n",
    "\n",
    "#load_document: fonction utile, qui prend un chemin d'entrée determine son extension, initialise le load approprié depuis DocumentLoader mapping, et retourne le contenu sous forme de liste d'Objet Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee59f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpubReader(UnstructuredEPubLoader):\n",
    "    def __init__(self, file_path: str | list[str], **unstructured_kwargs:Any):\n",
    "        super().__init__(file_path, **unstructured_kwargs, mode=\"elements\", strategy=\"fast\")\n",
    "\n",
    "class DocumentLoaderException(Exception):\n",
    "    pass\n",
    "\n",
    "class DocumentLoader(object):\n",
    "    \"\"\"Loads in a document with a supported extension\"\"\"\n",
    "\n",
    "    supported_extensions = {\n",
    "        \".pdf\": PyPDFLoader,\n",
    "        \".txt\": TextLoader,\n",
    "        \".epub\":EpubReader,\n",
    "        \".docx\": UnstructuredWordDocumentLoader,\n",
    "        \".doc\": UnstructuredWordDocumentLoader,\n",
    "    }\n",
    "\n",
    "def load_document(temp_filepath:str) -> list[Document]:\n",
    "    \"\"\"Load le fichier et retourne une liste de documents.\"\"\"\n",
    "    ext = pathlib.Path(temp_filepath).suffix\n",
    "    loader = DocumentLoader.supported_extensions.get(ext)\n",
    "    if not loader:\n",
    "        raise DocumentLoaderException(\n",
    "            f\"Invalid extension type {ext}, cannot load this type of file\"\n",
    "        )\n",
    "    loader = loader(temp_filepath)\n",
    "    docs = loader.load()\n",
    "\n",
    "def load_documents(temp_filepath:str) -> list[Document]:\n",
    "    \"\"\"Load a file and return it as a list of documents.\"\"\"\n",
    "    ext = pathlib.Path(temp_filepath).suffix\n",
    "    loader = DocumentLoader.supported_extensions.get(ext)\n",
    "    if not loader:\n",
    "        raise DocumentLoaderException(\n",
    "            f\"Invalid extension type {ext}, cannot load this type of file\"\n",
    "        )\n",
    "    loaded = loader(temp_filepath)\n",
    "    docs = loaded.load()\n",
    "    logging.info(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15b0653",
   "metadata": {},
   "source": [
    "#LLM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee2141",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CacheBackedEmbeddings' from 'langchain_core.embeddings' (/home/eric/.cache/pypoetry/virtualenvs/rag-langchain-tuto-5V8OYf-9-py3.12/lib/python3.12/site-packages/langchain_core/embeddings/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#LLM.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CacheBackedEmbeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstorage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LocalFileStore\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'CacheBackedEmbeddings' from 'langchain_core.embeddings' (/home/eric/.cache/pypoetry/virtualenvs/rag-langchain-tuto-5V8OYf-9-py3.12/lib/python3.12/site-packages/langchain_core/embeddings/__init__.py)"
     ]
    }
   ],
   "source": [
    "#LLM.py\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import OpenAiEmbeddings\n",
    "\n",
    "from config import set_environment\n",
    "\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c87b6d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LocalFileStore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m store = \u001b[43mLocalFileStore\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m./cache/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m underlying_embeddings = OpenAiEmbeddings(\n\u001b[32m      3\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-large\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#Avoiding unnecessary costs by caching the embeddings.\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'LocalFileStore' is not defined"
     ]
    }
   ],
   "source": [
    "store = LocalFileStore(\"./cache/\")\n",
    "underlying_embeddings = OpenAiEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "\n",
    "#Avoiding unnecessary costs by caching the embeddings.\n",
    "EMBEDDINGS = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")\n",
    "#Reduit le coût API et accelere les requetes répétitives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca9bb7",
   "metadata": {},
   "source": [
    "#Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc9c81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import List, Any\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#from chapter4.document_loader import load_document\n",
    "#from chapter4.llms import EMBEDDINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e343727",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EMBEDDINGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m VECTOR_STORE = InMemoryVectorStore(embedding=\u001b[43mEMBEDDINGS\u001b[49m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#Les documents chunkés sont stockés dans un InMemoryVectorStore en utilisant le cache embedding permettant de faire de rapides checke de similarités\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'EMBEDDINGS' is not defined"
     ]
    }
   ],
   "source": [
    "VECTOR_STORE = InMemoryVectorStore(embedding=EMBEDDINGS)\n",
    "#Les documents chunkés sont stockés dans un InMemoryVectorStore en utilisant le cache embedding permettant de faire de rapides checke de similarités\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4c0070c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_documents\u001b[39m(docs: \u001b[43mList\u001b[49m[Document]) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Split each document.\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m     text_splitter = RecursiveCharacterTextSplitter(\n\u001b[32m      4\u001b[39m         chunck_size=\u001b[32m1500\u001b[39m, chunck_overlap=\u001b[32m200\u001b[39m\n\u001b[32m      5\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def split_documents(docs: List[Document]) -> list[Document]:\n",
    "    \"\"\"Split each document.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunck_size=1500, chunck_overlap=200\n",
    "    )\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20dc1058",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseRetriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ce retriever custom herite d'un retriever de base et gère la liste de doc interne\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDocumentRetriever\u001b[39;00m(\u001b[43mBaseRetriever\u001b[49m):\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mA retriever that contains the top k documents that contain the user query\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m     documents: List[Document] = []\n",
      "\u001b[31mNameError\u001b[39m: name 'BaseRetriever' is not defined"
     ]
    }
   ],
   "source": [
    "# Ce retriever custom herite d'un retriever de base et gère la liste de doc interne\n",
    "class DocumentRetriever(BaseRetriever):\n",
    "    \"A retriever that contains the top k documents that contain the user query\"\n",
    "    documents: List[Document] = []\n",
    "    k: int = 5\n",
    "\n",
    "    def model_post_init(self, ctx: Any) -> None:\n",
    "        self.store_documents(self.documents)\n",
    "\n",
    "    @staticmethod\n",
    "    def store_documents(docs: List[Document]) -> None:\n",
    "        \"\"\"Add documents to the vector store.\"\"\"\n",
    "        splits = split_documents(docs)\n",
    "        VECTOR_STORE.add_documents(splits)\n",
    "\n",
    "    def add_uploaded_docs(self, uploaded_files):\n",
    "        \"\"\"Add uploaded documents\"\"\"\n",
    "        docs = []\n",
    "        temp_dir = tempfile.TemporaryDirectory()\n",
    "        for file in uploaded_files:\n",
    "            temp_filepath = os.path.join(temp_dir.name, file.name)\n",
    "            with open(temp_filepath, \"wb\") as f:\n",
    "                f.write(file.getvalue())\n",
    "                docs.extend(load_document(temp_filepath))\n",
    "        self.documents.extend(docs)\n",
    "        self.store_documents(docs)\n",
    "\n",
    "    def _get_relevant_documents(self, query, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "        if len(self.documents) == 0:\n",
    "            return []\n",
    "        return VECTOR_STORE.similarity_search(query=query, k=self.k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba9649",
   "metadata": {},
   "source": [
    "#Designing the state graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import END\n",
    "from langgraph.graph import START, StateGraph, add_messages\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "#from chapter4.llms import chat_model\n",
    "#from chapter4.retriever import DocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"\"\"Tu es un assistant intelligent spécialisé dans l’analyse de profils professionnels.\n",
    "    Ton rôle est d’aider un utilisateur à comprendre si son expérience, ses compétences et ses attentes correspondent à ses objectifs de carrière.\n",
    "    Tu dois :\n",
    "    - Lire et analyser toutes les informations fournies sur le profil de l’utilisateur (CV, projets, compétences, expériences, objectifs).\n",
    "    - Évaluer de manière objective la correspondance entre les compétences et les aspirations professionnelles de l’utilisateur et le type de poste ou de mission qu’il recherche.\n",
    "    - Fournir des recommandations claires pour positionner le profil de manière pertinente : points forts à mettre en avant, compétences à valoriser, éventuels écarts à combler.\n",
    "    - Être concis, structuré et orienté action, comme un conseiller de confiance pour un recrutement ou une évolution de carrière.\n",
    "    - Ne jamais inventer de compétences ou d’expériences qui n’existent pas dans les données fournies.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = DocumentRetriever()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt)\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd31528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    issues_report: bool\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf19779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = retriever.invoke(state[\"messages\"][-1].content)\n",
    "    print(retrieved_docs)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke(\n",
    "        {\"question\": state[\"messages\"][-1].content, \"context\": docs_content}\n",
    "    )\n",
    "    response = chat_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "def double_check(state: State):\n",
    "    result = chat_model.invoke(\n",
    "        [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Review the following project documentation for compliance with our corporate standards.\"\n",
    "                f\"Return 'ISSUES FOUND' followed by any detected or 'NO ISSUES': {state['answer']}\"\n",
    "            )\n",
    "        }]\n",
    "    )\n",
    "    if \"ISSUES FOUND\" in result.content:\n",
    "        print(\"issues detected\")\n",
    "        return {\n",
    "            \"issues_report\": result.split(\"ISSUES FOUND\", 1)[1].strip(),\n",
    "        }\n",
    "    print(\"no issues detected\")\n",
    "    return {\n",
    "        \"issues_report\": \"\"\n",
    "        \"issues_detected\": False\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_finalizer(state: State):\n",
    "    \"\"\"Finalise la documentation en intégrant les feedbacks\"\"\"\n",
    "    if \"issues_detected\" in state and state[\"issue_detected\"]:\n",
    "        response = chat_model.invoke(\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Revise the following documentation to address these feedback points: {state['issues_report']}\\n\"\n",
    "                    f\"Original Document: {state['answer']}\\n\"\n",
    "                    f\"Always return the full revised document, even if no changes are needed.\"\n",
    "                )\n",
    "            }]\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [AIMessage(response.content)]\n",
    "        }\n",
    "    return {\n",
    "        \"messages\": [AIMessage(state[\"answer\"])]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence(\n",
    "    [retriever, generate, double_check, doc_finalizer]\n",
    ")\n",
    "graph_builder.add_edge(START, \"retriever\")\n",
    "graph_builder.add_edge(\"doc_finalizer\", END)\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f721da",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "input_messages = [HumanMessage(\"What's the square root of 10?\")]\n",
    "response = graph.invoke({\"messages\": input_messages}, config=config)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-langchain-tuto-5V8OYf-9-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
